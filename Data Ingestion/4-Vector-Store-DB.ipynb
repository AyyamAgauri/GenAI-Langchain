{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Store DB are essential to store the embeddings vector of the input text formed by the Embedding Models. There are many VectorDB such as FAISS, PineconeDB, ChromaDB, AstraDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('Sample-Transformers.txt')\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Splitting into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 369, which is longer than the specified 200\n",
      "Created a chunk of size 335, which is longer than the specified 200\n",
      "Created a chunk of size 312, which is longer than the specified 200\n",
      "Created a chunk of size 318, which is longer than the specified 200\n",
      "Created a chunk of size 698, which is longer than the specified 200\n",
      "Created a chunk of size 260, which is longer than the specified 200\n",
      "Created a chunk of size 265, which is longer than the specified 200\n",
      "Created a chunk of size 364, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "splitter = CharacterTextSplitter(chunk_size = 200,chunk_overlap = 10)\n",
    "docs = splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings created of Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='gemma2')\n",
    "doc_embeddings = embeddings.embed_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector DB created with Stored Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='### Key Components of the Transformer'),\n",
       " Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='By enabling more efficient parallelization and capturing complex dependencies within text, Transformers have set new benchmarks and opened up new possibilities for language understanding and generation.'),\n",
       " Document(metadata={'source': 'Sample-Transformers.txt'}, page_content=\"4. **Encoder-Decoder Structure**:\\n   The Transformer consists of two main parts: the encoder and the decoder. \\n   - **Encoder**: The encoder is composed of a stack of identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The encoder processes the input sequence and generates a continuous representation.\\n   - **Decoder**: The decoder is also composed of a stack of identical layers, but each layer has an additional sub-layer to perform multi-head attention over the encoder's output. The decoder generates the output sequence by attending to both the encoder's output and the previously generated tokens.\"),\n",
       " Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='6. **Layer Normalization and Residual Connections**:\\n   Each sub-layer (both in the encoder and the decoder) is followed by layer normalization and a residual connection. These techniques help stabilize the training process and improve the convergence of the model.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(query='what is a transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='### Key Components of the Transformer'),\n",
       "  8133.539),\n",
       " (Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='By enabling more efficient parallelization and capturing complex dependencies within text, Transformers have set new benchmarks and opened up new possibilities for language understanding and generation.'),\n",
       "  10672.725),\n",
       " (Document(metadata={'source': 'Sample-Transformers.txt'}, page_content=\"4. **Encoder-Decoder Structure**:\\n   The Transformer consists of two main parts: the encoder and the decoder. \\n   - **Encoder**: The encoder is composed of a stack of identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The encoder processes the input sequence and generates a continuous representation.\\n   - **Decoder**: The decoder is also composed of a stack of identical layers, but each layer has an additional sub-layer to perform multi-head attention over the encoder's output. The decoder generates the output sequence by attending to both the encoder's output and the previously generated tokens.\"),\n",
       "  11067.215),\n",
       " (Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='6. **Layer Normalization and Residual Connections**:\\n   Each sub-layer (both in the encoder and the decoder) is followed by layer normalization and a residual connection. These techniques help stabilize the training process and improve the convergence of the model.'),\n",
       "  11625.566)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score(query='What is a transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and Loading FAISS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"FAISS_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db = FAISS.load_local('FAISS_DB',embeddings,allow_dangerous_deserialization=True)\n",
    "\n",
    "new_docs = new_db.similarity_search(query='What is a transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='### Key Components of the Transformer'),\n",
       " Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='By enabling more efficient parallelization and capturing complex dependencies within text, Transformers have set new benchmarks and opened up new possibilities for language understanding and generation.'),\n",
       " Document(metadata={'source': 'Sample-Transformers.txt'}, page_content=\"4. **Encoder-Decoder Structure**:\\n   The Transformer consists of two main parts: the encoder and the decoder. \\n   - **Encoder**: The encoder is composed of a stack of identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The encoder processes the input sequence and generates a continuous representation.\\n   - **Decoder**: The decoder is also composed of a stack of identical layers, but each layer has an additional sub-layer to perform multi-head attention over the encoder's output. The decoder generates the output sequence by attending to both the encoder's output and the previously generated tokens.\"),\n",
       " Document(metadata={'source': 'Sample-Transformers.txt'}, page_content='6. **Layer Normalization and Residual Connections**:\\n   Each sub-layer (both in the encoder and the decoder) is followed by layer normalization and a residual connection. These techniques help stabilize the training process and improve the convergence of the model.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same commands are for ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
